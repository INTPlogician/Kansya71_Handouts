---
title: resume:「J-POP」のトレンド1960-2019
subtitle: ---構造的トピックモデルによる推定---
author: "小牧和哉（大阪大学 人間科学研究科M1）"
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
    number_sections: true
    toc: true
    includes: 
      in_header: preamble_latex.tex
  documentclass: bxjsarticle
  classoption: xelatex,ja=standard
always_allow_html: true 
 
    
---

```{r,include=F}



knitr::opts_chunk$set(
	echo = F,
	message = FALSE,
	eval=T,
	include=T,
	error = F,
	warning = FALSE,
	prompt=T,
	fig.width = 7,
	fig.height=4,
	fig.align="center",
	dpi=120,
  results="axis",
	dev = "cairo_pdf",
  dev.args = list(family = "ipaexg")
	

)

load(".RData")
library(DiagrammeR)
library(knitr)
library(rmdformats)
library(RMeCab)
library(dplyr)
library(stringr)
library(stringi)
library(ggplot2)
library(magrittr)
library(stm)
library(devtools)
library(usethis)
library(readxl)
library(readtext)
library(tidyr)
library(tm)
library(tibble)
library(tidytext)
library(tidyverse)
library(ggthemes)
library(furrr)
library(scales)
library(corrplot)
library(rmarkdown)
library(kableExtra)



```
<!--word_document:
    highlight: tango
    fig_width: 2
    fig_height: 1.5
    fig_caption: true	
  beamer_presentation:
    incremental: true
    pandoc_args: [--latex-engine=lualatex, --include-in-header=preamble.tex]
    fig_width: 3
    fig_height: 2
  revealjs_presentation:
    incremental: true
    transition: cube
    fig_width: 3
    fig_height: 2
  ioslides_presentation:
    incremental: true
    transition: faster
    fig_width: 3
    fig_height: 2 
     html_document:
    toc: true
    theme: united
    fig_width: 3
    fig_height: 2
-->

# はじめに

　本発表の目的は, 1960~2019の歌詞のテキストデータを用いて, 「J-POP」の**トピック**変遷を, **構造的トピックモデル**により推定することである。
　



# 方法

## 分析モデル

　本発表では, トピックモデルを分析に用いた。トピックモデルにおいて幾分か簡略化して説明をしておく。　\newline
　トピックモデルは, クラスタリングにおいて, 特定の文書が1トピック「のみ(single)」に分類されるのではなく, 「複数の(mixed)」トピックに潜在的・確率的に分布していると考えるモデルである。
トピックモデルは細分類として, LDA(Latent Dirichlet Allocation), CTM(Correlated Topic Model)等が存在するが, ここで用いたのはトピック同士に相関を仮定し, ディリクレ分布などの事前分布の代わりに, 共変量(covariates)を投入して回帰モデルに近似させるSTM(Structural Topic Model)を用いた。　\newline
　STMの数式的な表現は以下の通りである。

トピックの割合(Topic Prevalence)における事前分布仮定: 

$$μ_{d,k}=X_{dγk}$$ 　\newline                                   　
$$γ_{k}\sim Normal (0,σ_k^2 ),    for ~ \ k=1…K-1$$ 　\newline
$$σ_k^2\sim Gamma (s^γ,r^γ )$$     　\newline

　ここでは, トピック分布がガンマ分布として仮定されている。

確率論的言語モデルにおける分布仮定:

$$  \vec{\theta_d}\sim LogisticNormal (\Gamma^\prime X_d^\prime,\sum)$$    　\newline
$$z_{d,n}\sim  Multinomial(\theta_d), for \ n=1...N_d\\$$  　\newline
$$w_{d,n}\sim  Multinomial(\beta_{z_{d,n}}),  for \ n=1...N_d  $$ 　\newline

　ここで言う$θ_d$は,文書ごとのトピックの出現確率分布のことを指しており, $Γ^T=[ γ_1… γ_k]$のdocument(文書)×$K$ (トピック数)-1の係数行列で表現される。$Σ$はトピックの分散共分散行列を指している。これらのパラメーターは, ロジステイック正規分布に従うことを仮定しており, Softmax関数により, 合計は「1」となる。
そして, この, document($d$)中の単語($n$)単位における潜在トピック$z_{d,n  }$は$θ_d$をパラメーターとした多項分布に従う。
　最後に, $z_{d,n}$によって生成された観察者の手元にある観察される単語$w_{d,n}$は, 潜在トピックが割り当てられた単語分布$β_{z_{d,n}}$をパラメーターとした多項分布に従う。


トピックの中身における分布仮定:

$$β_{d,v}^k   ∝  exp (m_v+ k_v^y+ k_v^k  + k_v^{y,k})$$ 　\newline
$$k_v^{y,k}   \sim Laplace(0,τ_v^{y,k} ) $$   　\newline
$$τ_v^{y,k}   \sim Gamma(s^k,r^k)   $$   　\newline

　トピック$K$が与えられた状態でのドキュメントに出現する単語の確率分布$β_{d,v}^k$は, 単語の出現率の対数分布を表すパラメーター$m_v$, トピック独自の分散$k_v^y$, 共変量の分散$k_v^k$ そしてそれらの交互作用による分散$k_v^(y,k)$によって表現される。このモデルは通称SAGE（Sparse Additive Generative Model）と呼ばれる推定手法である。$β$の事前分布を新たに仮定することなく, トピックの固有性を対数加法モデルで表現することによって計算量を削減できることが特徴である。分散はラプラス分布によってスムージングされており, スパースなデータにおいても推定値のロバスト性を維持できる。ちなみに, 共変量が用意されていない場合はトピックの確率を単純推定するのみとなる。
　これらの階層的な分布仮定を置いたうえで, STMは, 上記に挙げた文書ごとのトピック比率$θ_d$と単語の出現確率分布$β_{d,v}^k$の両パラメーターに加え, 外生変数である共変量（文書が生成された年, 著者, 文書の分類, 評価）で構造化し（structuralize）, 数学的なアルゴリズム手法(変分ベイズ, マルコフ連鎖モンテカルロ, EM等)を用いて, モデル収束convergenceさせることによって, 推定値を算出する。



## 記述統計量

```{r, results="asis"}

knitr::kable(summary(sampled2),caption="記述統計量")  %>% 
 kable_styling(full_width = F,latex_options = c("hold_position", "scale_down")) %>%
  row_spec(0,bold=T, align ="c") %>%
   column_spec(2,width = "2em") 




```

# 結果

## トピック数（K）診断: 図の補足

```{r}
kResult$results %>%
  select(K, exclus, semcoh,heldout,residual,lbound,em.its) %>%
   filter(K %in% 2:50) %>%
    unnest() %>%  
     round(digits=2) %>% 
      mutate(K = as.factor(K)) 

```

```{r}

knitr::kable(modelc)

```


### Semantic Coherence

 数式的には以下のようになる(Mimno, D., et. al., 2011)。

$$C(K;V^{(k)}  )=  \sum_{m=2}^M\sum_{l=1}^{m-1} log\frac{ D(v_m^{(k)} ,v_l^{(k)}  )+1)}{D(v_l^{(k)} )}$$

　$D(v)$は$v$という単語が出現する文書の頻度, $D(v_m^{(k)} ,v_l^{(k)}  )+1)$は単語$v$と$v'$が共起する文書の頻度を示す。なお, $V^k=(v_1^k,…,v_M^k)$はトピック$K$において, $M$という単語が最も出現する確率である単語のリストである。数式中の「1」は0の場合の対数値が計算できるように加えてある。
　簡単に言えば, 共起語の条件つき確率の総和を求めて, それを意味論的なつながりの指標として捉えていることが分かる。
  
### Exclusivity
  
  数式としては以下のようになる(Bischof and Airoldi, 2012)。

$$φ_{f,k}=  \frac{β_{f,k}}{(∑_{j∈S}β_{f,j} )}$$

$β_{f,k}$はトピックKにおける単語の頻度を指し, $∑_{j∈S}β_{f,j}$ はトピック集合$S$における$j$トピックにおける単語の頻度の総和を意味している。

### 各トピックの頻出単語

```{r}

labelTopics(poliblogSelect7topic, c(1:7))





```
 なお, それぞれの指標の算出式は以下の通りである。
 
 $$Lift= \frac{\beta_{k,v}}{w_v/\Sigma_v w_v}$$
 
 $v$は単語, $k$はトピックを示す。トピックにおける単語分布を, 単語出現率でわったもの。
 
 $$FREX_{k,v}=(\frac{\omega}{FCDF(\beta_{k,v}/\Sigma_{j=1}^K \beta_{j,v})}+\frac{1-\omega}{ECDF(\beta_{k,v})})^{-1}$$
 
 トピックにおける頻繁かつ排他的な単語を示す指標。単語のランクの調和平均。
 
 $$SCORE=\beta_{v,k}(log\beta_{w,k}-1/K\Sigma_k\prime log\beta_{v,k\prime})$$
 
 

## J-POPのトレンド

 回帰分析の結果を以下に示す。

### 経過年をダミー変数として投入した場合

```{r}

options(scipen=20)

summary(kaiki7.lm)


```

### スプライン関数をあてはめた場合


```{r}

options(scipen=20)

summary(kaiki7.spline)


```


## 共変量ごとのトレンド推定: 歌手性別


```{r,tidy=TRUE}

options(scipen=20)

summary(prep2)

```

## 共変量ごとのトレンド推定: 作詞者性別

```{r}

options(scipen=20)


summary(prep3)


```






### 補足: 多重共線性について

 本来, 説明変数間の多重共線性が通常の回帰モデルでは発生するが, 機械学習においては, L1正則化ペナルティ項を加えることで(回帰モデルのパラメーター推定でいえば, Lasso回帰にこれがあたる), 次元圧縮を行い, 多重共線性を回避できる。トピックモデルにおける正則化式は以下のようになる(Roberts, M. E., et. al, 2016, p. 18)。

$$argminS= ∑_v| β_{k,v}^{ref}  -  β_{k,v}^{cand} |$$

$argminS$は, 誤差を最小化する損失関数, $β_{k,v}^{ref}$はターゲットモデル,   $β_{k,v}^{cand}$は推定モデルを指す。


# 引用文献

Bischof and Airoldi. (2012). Summarizing topical content with word frequency and exclusivity. *In Proceedings of the International Conference on Machine Learning*.　\newline
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation.　*Journal of machine Learning research*, 3, 993-1022. 　\newline
北田暁大 (2004). <意味>への抗い メディエーションの文化政治学　せりか書房 　\newline
北田暁大(2005). 嗤う日本の「ナショナリズム」　日本放送出版協会 　\newline
増田 聡 (2006). 聴衆をつくる 音楽批評の解体文法 青土社 　\newline
Mimno, D., Wallach, HM., Talley, E., Leenders, M. & McCallum, A. (2011). Optimizing Semantic Coherence in Topic Models. *In Proceedings of the Conference on Empirical Methods in Natural Language Processing*, EMNLP ’11, 262–272. 　\newline
見田宗介 (1978). 近代日本の心情の歴史―流行歌の社会心理史　講談社 　\newline
小川博司 (1997). ポピュラー音楽研究の困難と課題　ポピュラー音楽研究, 1, 2-6. 　\newline
大出 彩・松本文子・金子貴昭 (2013). 流行歌から見る歌詞の年代別変化 情報処理学会, 4, 103‒110. 　\newline
Roberts, E. M., Stewart, B. M., Tingley, D., Lucas, C., Leder-Luis, J., Gadarian, S. K., Albertson, B. & Rand, D. G. (2014). Structual Topic Models for Open-Ended Survey Responses. *American Journal of Political Science*, 58(4), 1064-1082. 　\newline
Roberts, E. M., Stewart, B. M. & Airoldi, E. M. (2016). A Model of Text for Experimentation in the Social Sciences. *Journal of the American Statistical Association*, 111(515), 988-1003. 　\newline
Roberts, M. E., Stewart, B. M. & Tingley, S. D. (2019). stm: An R Package for Structural Topic Models. *Journal of Statistical Software*, 91(2), 1-40. 　\newline
烏賀陽弘道(2005). Jポップとは何か　岩波書店 　\newline














